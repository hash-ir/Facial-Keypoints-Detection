{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Facial Keypoints Detection\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torchvision\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torchvision import models\n",
    "from torchvision import transforms\n",
    "from torch.utils.data import DataLoader, SequentialSampler\n",
    "from data_loader import FacialKeypointsDataset\n",
    "import transform\n",
    "import matplotlib.pyplot as plt\n",
    "import time\n",
    "import copy\n",
    "import numpy as np\n",
    "import cv2 as cv\n",
    "\n",
    "print('Torch version: {}'.format(torch.__version__))\n",
    "print('Torchvision version: {}'.format(torchvision.__version__))\n",
    "device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
    "print('Using GPU: {}'.format(device))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load the dataset with the following transforms:\n",
    "* Normalization in the range (a, b)\n",
    "\\begin{equation}\n",
    "sample^* = (b - a) * \\frac{(sample - min(sample))}{max(sample) - min(sample)} + a\n",
    "\\end{equation}\n",
    "\n",
    "     \\begin{equation} Image = (1-0) * \\frac{Image - 0}{255 - 0} + 0 = Image/255 \\end{equation}\n",
    "     \\begin{equation} Keypoints = (1 - (-1))* \\frac{Keypoints - 0}{96 - 0} + (-1) = \\frac{Keypoints - 48}{48} \\end{equation} \n",
    "\n",
    "* Numpy array to Tensor conversion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_transforms = transforms.Compose([\n",
    "    transform.Normalize(),\n",
    "    transform.ToTensor(),\n",
    "])\n",
    "train_set = FacialKeypointsDataset('training.csv', transform=data_transforms)\n",
    "val_set = FacialKeypointsDataset('val.csv', transform=data_transforms)\n",
    "print(len(train_set), len(val_set))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Use PyTorch DataLoader to load streams of train and validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bs = 20\n",
    "train_loader = DataLoader(train_set, batch_size=bs, shuffle=True, num_workers=4)\n",
    "val_loader = DataLoader(val_set, batch_size=bs, shuffle=False, num_workers=4, drop_last=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plot some samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_samples = 4\n",
    "fig = plt.figure(figsize=(15,15))\n",
    "for idx, sample in enumerate(train_loader):\n",
    "    if idx == num_samples:\n",
    "        break\n",
    "    image, key_pts = sample['image'], sample['keypoints']\n",
    "    image = torch.squeeze(image[0]) * 255.\n",
    "    key_pts = (key_pts[0].view(-1, 2) * 48) + 48\n",
    "    plt.subplot(1, num_samples ,idx+1)\n",
    "    plt.title('Sample {}'.format(idx))\n",
    "    plt.axis('off')\n",
    "    plt.imshow(image, cmap='gray')\n",
    "    plt.scatter(key_pts[:,0], key_pts[:,1], s=100,  marker='.', c='r')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define Network Architecture\n",
    "```python\n",
    "features{(Layer Segment 1) = Conv2d(32, 96, 96),\n",
    "                             BatchNorm2d(32, 96, 96),\n",
    "                             MaxPool2d(32, 48, 48),\n",
    "                             ReLU(32, 48, 48),\n",
    "                             Dropout2d(32, 48, 48)\n",
    "                \n",
    "         (Layer Segment 2) = Conv2d(64, 48, 48),\n",
    "                             BatchNorm2d(64, 48, 48),\n",
    "                             MaxPool2d(64, 24, 24),\n",
    "                             ReLU(64, 24, 24),\n",
    "                             Dropout2d(64, 24, 24)\n",
    "                \n",
    "         (Layer Segment 3) = Conv2d(128, 24, 24),\n",
    "                             BatchNorm2d(128, 24, 24),\n",
    "                             MaxPool2d(128, 12, 12),\n",
    "                             ReLU(128, 12, 12),\n",
    "                             Dropout2d(128, 12, 12)\n",
    "\n",
    "         (Layer Segment 4) = Conv2d(256, 12, 12),\n",
    "                             BatchNorm2d(256, 12, 12),\n",
    "                             MaxPool2d(256, 6, 6),\n",
    "                             ReLU(256, 6, 6),\n",
    "                             Dropout2d(256, 6, 6)\n",
    "         }\n",
    "                \n",
    "classifier{(Layer Segment 1) = Linear(256),\n",
    "                               ReLU(256),\n",
    "                               Dropout(256),\n",
    "                               Linear(30)\n",
    "          }\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class KeypointsNet(nn.Module):\n",
    "\n",
    "    def __init__(self):\n",
    "        super(KeypointsNet, self).__init__()\n",
    "        self.reset()\n",
    "        self.features = nn.Sequential(\n",
    "            nn.Conv2d(1, 32, 3, padding=1),\n",
    "            nn.BatchNorm2d(32),\n",
    "            nn.MaxPool2d(2, 2),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout2d(0.1),\n",
    "\n",
    "            nn.Conv2d(32, 64, 3, padding=1),\n",
    "            nn.BatchNorm2d(64),\n",
    "            nn.MaxPool2d(2, 2),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout2d(0.2),\n",
    "\n",
    "            nn.Conv2d(64, 128, 3, padding=1),\n",
    "            nn.BatchNorm2d(128),\n",
    "            nn.MaxPool2d(2, 2),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout2d(0.3),\n",
    "\n",
    "            nn.Conv2d(128, 256, 3, padding=1),\n",
    "            nn.BatchNorm2d(256),\n",
    "            nn.MaxPool2d(2, 2),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout2d(0.4),\n",
    "        )\n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Linear(256*6*6, 256),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.4),\n",
    "            nn.Linear(256, 30)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.features(x)\n",
    "        x = x.view(-1, self.num_flat_features(x))\n",
    "        x = self.classifier(x)\n",
    "        return x\n",
    "    \n",
    "    def num_flat_features(self, x):\n",
    "        n_feats = 1\n",
    "        for s in x.size()[1:]:\n",
    "            n_feats *= s\n",
    "\n",
    "        return n_feats\n",
    "\n",
    "    def reset(self):\n",
    "        self.train_loss_history = []\n",
    "        self.train_acc_history = []\n",
    "        self.val_loss_history = []\n",
    "        self.val_acc_history = []\n",
    "\n",
    "    def print_params(self):\n",
    "        total_params = sum(param.numel() for param in self.parameters())\n",
    "        print('Total params: {}'.format(total_params))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Display Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = KeypointsNet()\n",
    "print(model)\n",
    "model.print_params()\n",
    "model = model.to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training Loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_net(model, train_loader, val_loader, use_cuda, print_every, n_epochs):\n",
    "    print('\\nStarted training...\\n')\n",
    "    best_score = 140\n",
    "    best_model_wts = copy.deepcopy(model.state_dict())\n",
    "    for epoch in range(n_epochs):\n",
    "        model.train\n",
    "        running_loss = 0.0\n",
    "        running_corrects = 0\n",
    "        for batch, data in enumerate(train_loader):\n",
    "            images, key_pts = data['image'], data['keypoints']\n",
    "            key_pts = key_pts.view(key_pts.size(0), -1)\n",
    "            if use_cuda:\n",
    "                images = images.to(device)\n",
    "                key_pts = key_pts.to(device)\n",
    "\n",
    "            output = model(images)\n",
    "            train_loss = criterion(output, key_pts)\n",
    "            model.train_loss_history.append(train_loss.item())\n",
    "            \n",
    "            optimizer.zero_grad()\n",
    "            train_loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            if (batch+1) % print_every == 0:\n",
    "                print('[Iteration {}/{}] Training loss: {:.3f}'.format(batch+1, len(train_loader), train_loss.item()))\n",
    "\n",
    "        with torch.no_grad():\n",
    "            running_loss = 0.0\n",
    "\n",
    "            model.eval\n",
    "            for batch, data in enumerate(val_loader):\n",
    "                image_val, key_pts_val = data['image'], data['keypoints']\n",
    "                key_pts_val = key_pts_val.view(key_pts_val.size(0), -1)\n",
    "                if use_cuda:\n",
    "                    image_val = image_val.to(device)\n",
    "                    key_pts_val = key_pts_val.to(device)\n",
    "                    \n",
    "\n",
    "                o_val = model(image_val)\n",
    "                val_loss = criterion(o_val, key_pts_val)\n",
    "                running_loss += val_loss.item()\n",
    "                model.val_loss_history.append(val_loss.item())\n",
    "                \n",
    "        metric = 1.0 / (2 * (running_loss/len(val_loader)))\n",
    "        print('[Epoch {}/{}] Training loss: {:.3f}, Validation score: {:.3f}\\n'.format(epoch+1, n_epochs, model.train_loss_history[-1], metric))\n",
    "\n",
    "        if metric > best_score:\n",
    "            best_score = metric\n",
    "            best_model_wts = copy.deepcopy(model.state_dict())\n",
    "            print('Achieved better score. Saving state...')\n",
    "\n",
    "    print('\\nFinished training...\\n')\n",
    "    model.load_state_dict(best_model_wts)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "since = time.time()\n",
    "criterion = nn.MSELoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.01, betas=(0.9, 0.999), eps=1e-8, weight_decay=1e-6)\n",
    "best_model = train_net(model, train_loader, val_loader, True, 10, 25)\n",
    "torch.save(best_model, 'bestModel2.pt')\n",
    "time_elapsed = time.time() - since\n",
    "print('Total training time: {}m {}s'.format(time_elapsed // 60, time_elapsed % 60))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Testing\n",
    "\n",
    "* Detect faces using OpenCV's Haar Cascade\n",
    "* Resize detected faces to (96, 96)\n",
    "* Make a Torch Tensor from the face sample\n",
    "* Evaluate the model on the sample\n",
    "* Plot the output keypoints back on the sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "face_cascade = cv.CascadeClassifier('/home/hashir/anaconda3/lib/python3.7/site-packages/cv2/data/haarcascade_frontalface_default.xml')\n",
    "\n",
    "def face_detect(frame):\n",
    "    gray = cv.cvtColor(frame, cv.COLOR_BGR2GRAY)\n",
    "    faces = face_cascade.detectMultiScale(gray, 1.3, 5)\n",
    "    face_gray = None\n",
    "    for (x,y,w,h) in faces:\n",
    "        cv.rectangle(frame,(x,y),(x+w,y+h),(255,0,0),2)\n",
    "        face_gray = gray[y:y+h, x:x+w]\n",
    "    \n",
    "    if face_gray is not None:\n",
    "        sample = cv.resize(face_gray, (96, 96))\n",
    "        sample_np = np.asarray(sample).reshape(1,96,96).astype(np.float32)\n",
    "        sample_np /= 255.\n",
    "        sample_tensor = torch.from_numpy(sample_np).unsqueeze(0).to(device)\n",
    "        return sample_tensor\n",
    "    else:\n",
    "        print('No face found!')\n",
    "\n",
    "saved_model = torch.load('Saved_Models/bestModel2.pt')\n",
    "saved_model = saved_model.to(device)\n",
    "saved_model.eval()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test in real-time using OpenCV's Video Capture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cap = cv.VideoCapture(0)\n",
    "time.sleep(2.0)\n",
    "\n",
    "# fourcc = cv.VideoWriter_fourcc(*'MJPG')\n",
    "# out = cv.VideoWriter('output.avi',fourcc, 20.0)\n",
    "\n",
    "while cap.isOpened():\n",
    "    ret, frame = cap.read()\n",
    "#     frame = cv.resize(frame, (1024, 600))\n",
    "    gray = cv.cvtColor(frame, cv.COLOR_BGR2GRAY)\n",
    "    faces = face_cascade.detectMultiScale(gray, 1.3, 5)\n",
    "    for (x,y,w,h) in faces:\n",
    "        cv.rectangle(frame,(x,y),(x+w,y+h),(255,0,0),2)\n",
    "        face_gray = gray[y:y+h, x:x+w]\n",
    "        original_shape = face_gray.shape\n",
    "        sample = cv.resize(face_gray, (96, 96))\n",
    "        sample = sample.astype('float32')/255\n",
    "        sample = np.asarray(sample).reshape(1,96,96)\n",
    "        sample = torch.from_numpy(sample).unsqueeze(0).to(device)\n",
    "        output = saved_model(sample)\n",
    "        output = output.view(-1, 2).detach()\n",
    "        output = (output * 48) + 48\n",
    "        output = output.cpu().numpy()\n",
    "        for i in range(15):\n",
    "            x_pt = (output[i][0] * (original_shape[1]/96)) + x\n",
    "            y_pt = (output[i][1] * (original_shape[0]/96)) + y\n",
    "            cv.circle(frame, (int(x_pt), int(y_pt)), 3, (0, 0, 255), -1)\n",
    "#     out.write(frame)\n",
    "    cv.imshow(\"Frame\", frame)\n",
    "    key = cv.waitKey(1) & 0xFF\n",
    "    \n",
    "    \n",
    "    if key == ord('q'):\n",
    "        break\n",
    "        \n",
    "cap.release()\n",
    "# out.release()\n",
    "cv.destroyAllWindows()\n",
    "cv.waitKey(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
